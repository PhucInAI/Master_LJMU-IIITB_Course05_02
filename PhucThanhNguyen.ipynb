{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7233b4df",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "<b> Use Tensorflow 2.10.0 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14f5cd",
   "metadata": {},
   "source": [
    "## 1. Data Loader and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e670de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import os, glob, shutil\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.layers import Dense, Dropout, Conv3D, Input, MaxPool3D, Flatten, Activation, \\\n",
    "                         TimeDistributed, BatchNormalization, MaxPooling2D, LSTM, GRU\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "# set random seed for whole project\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting global parameter\n",
    "data_folder = './data'\n",
    "checkpoints_folder = './checkpoints'\n",
    "\n",
    "# setting hyparameter for project\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "dest_size = (128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad7e3a",
   "metadata": {},
   "source": [
    "### 1.1 Read CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('./data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./data/val.csv').readlines()) \n",
    "train_df = pd.read_csv(os.path.join(data_folder, 'train.csv'), delimiter=';', names=['video', 'label (text)', 'label'])\n",
    "val_df   = pd.read_csv(os.path.join(data_folder, 'val.csv'), delimiter=';', names=['video', 'label (text)', 'label'])\n",
    "print('Total train samples :', len(train_df))\n",
    "print('Total val samples   :', len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e38ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at train and val\n",
    "print(train_df.head(5))\n",
    "print(val_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a labels dict with key is numberic and labels is text\n",
    "label_dict = {\n",
    "    0 : 'Left Swipe',\n",
    "    1 : 'Right Swipe',\n",
    "    2 : 'Stop',\n",
    "    3 : 'Thumbs Down',\n",
    "    4 : 'Thumbs Up',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c26a1c",
   "metadata": {},
   "source": [
    "### 1.2 Analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674db5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = train_df.groupby('label').size()\n",
    "data.plot.bar(xlabel='label', ylabel='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f00010",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = val_df.groupby('label').size()\n",
    "data.plot.bar(xlabel='label', ylabel='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size (Height and Width of Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889f0d0",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "<li> Data is quite balance </li>\n",
    "<li> Image size is very different between image </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508dbce",
   "metadata": {},
   "source": [
    "### 1.3 Data Generator and Transform\n",
    "Cause data is left and right sensitive, up and down sensitive, so it would be better to not clip, rotate it in spatial\n",
    "Cause data is sequence, so we should not random choose the frame order\n",
    "\n",
    "We can change the contrast and brightness of image\n",
    "https://docs.opencv.org/3.4/d3/dc1/tutorial_basic_linear_transform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(im, desired_size = dest_size[0]):\n",
    "    \"\"\"\n",
    "        Resize image with keep aspect ratio\n",
    "    \"\"\"\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    # new_size should be in (width, height) format\n",
    "\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "    delta_w = desired_size - new_size[1]\n",
    "    delta_h = desired_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return new_im\n",
    "\n",
    "def change_brightness(image):\n",
    "    alpha = np.random.uniform(0.75, 1.25)\n",
    "    beta = np.random.uniform(0, 100)\n",
    "    new_image = np.zeros(image.shape, image.dtype)\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            for c in range(image.shape[2]):\n",
    "                new_image[y,x,c] = np.clip(alpha*image[y,x,c] + beta, 0, 255)\n",
    "    return new_image\n",
    "\n",
    "def add_noise(image, threshold = 70):\n",
    "    \"\"\"\n",
    "        Add salt and pepper noise if get random int > threshold\n",
    "    \"\"\"\n",
    "    random_value = np.random.randint(0, 100)\n",
    "    if random_value > threshold:\n",
    "        image = image/255.0\n",
    "        row,col,ch = image.shape\n",
    "        s_vs_p = 0.5\n",
    "        amount = 0.004\n",
    "        out = np.copy(image)\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * image.size * s_vs_p)\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in image.shape]\n",
    "        out[coords] = 1\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in image.shape]\n",
    "        out[coords] = 0\n",
    "        out = image*255\n",
    "        return out\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e3c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "def generator(source_path, folder_list, batch_size, augmentation = False):\n",
    "    img_idx = [x for x in range(0,30,2)]#create a list of image numbers you want to use for a particular video, we are not using all the 30 images in a video, selected pickup of images\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches =int(len(t)/batch_size) # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            x=len(img_idx)\n",
    "            y=dest_size[0]\n",
    "            z=dest_size[1]\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            cnt_img=0\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = sorted(os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])) # read all the images in the folder\n",
    "                cnt_img+=1\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    if augmentation:\n",
    "                        image = change_brightness(image)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=resize_img(image)\n",
    "                    image=image/255.0\n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        cnt_img=0\n",
    "        batch_cover = num_batches*batch_size\n",
    "        rem = len(t) - batch_cover\n",
    "        if(len(t)!=batch_cover):\n",
    "            batch_data = np.zeros((rem,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((rem,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(rem):\n",
    "                imgs = sorted(os.listdir(source_path+'/'+ t[folder + batch_cover].split(';')[0])) # read all the images in the folder\n",
    "                cnt_img+=1\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in  \n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + batch_cover].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=resize_img(image)\n",
    "                    image=image/255.0\n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + batch_size].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "# def generator(source_path, folder_list, batch_size):\n",
    "#     print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "#     img_idx = [x for x in range(0,30,2)]#create a list of image numbers you want to use for a particular video, we are not using all the 30 images in a video, selected pickup of images\n",
    "#     while True:\n",
    "#         t = np.random.permutation(folder_list)\n",
    "#         num_batches =int(len(t)/batch_size) # calculate the number of batches\n",
    "#         for batch in range(num_batches): # we iterate over the number of batches\n",
    "#             x=len(img_idx)\n",
    "#             y=128\n",
    "#             z=128\n",
    "#             batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "#             batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "#             cnt_img=0\n",
    "#             for folder in range(batch_size): # iterate over the batch_size\n",
    "#                 imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "#                 cnt_img+=1\n",
    "#                 for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "#                     image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "#                     #crop the images and resize them. Note that the images are of 2 different shape \n",
    "#                     #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "#                     norm_image=image/255.0\n",
    "#                     image_resized=resize(norm_image,(128,128),mode='reflect')\n",
    "                    \n",
    "#                     batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])#normalise and feed in the image\n",
    "#                     batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])#normalise and feed in the image\n",
    "#                     batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "#                 batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "#             yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "#         # write the code for the remaining data points which are left after full batches\n",
    "#         cnt_img=0\n",
    "#         batch_cover = num_batches*batch_size\n",
    "#         rem = len(t) - batch_cover\n",
    "#         if(len(t)!=batch_cover):\n",
    "#             for folder in range(rem):\n",
    "#                 imgs = os.listdir(source_path+'/'+ t[folder + batch_cover].split(';')[0]) # read all the images in the folder\n",
    "#                 cnt_img+=1\n",
    "#                 for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "#                     image = imread(source_path+'/'+ t[folder + batch_cover].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "#                     #crop the images and resize them. Note that the images are of 2 different shape \n",
    "#                     #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "#                     norm_image = image/255.0 # normalizing the image \n",
    "#                     image_resized=resize(norm_image, (128,128), mode='reflect')\n",
    "                    \n",
    "#                     batch_data[folder,idx,:,:,0] = (image_resized[:,:,0]) #normalise and feed in the image\n",
    "#                     batch_data[folder,idx,:,:,1] = (image_resized[:,:,1]) #normalise and feed in the image\n",
    "#                     batch_data[folder,idx,:,:,2] = (image_resized[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "#                 batch_labels[folder, int(t[folder + batch_size].strip().split(';')[2])] = 1\n",
    "#             yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = './data/train'\n",
    "val_path = './data/val'\n",
    "num_train_sequences = len(train_df)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_df)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de1eb4",
   "metadata": {},
   "source": [
    "## Model buiding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, name, num_epochs = num_epochs, batch_size = batch_size, load_lastest = True):\n",
    "    # train and val generator\n",
    "    train_generator = generator(train_path, train_doc, batch_size)\n",
    "    val_generator = generator(val_path, val_doc, batch_size)\n",
    "    \n",
    "    # create folder for saving checkpoints\n",
    "    model_name = 'model' + '_' + name\n",
    "\n",
    "    if not os.path.exists(os.path.join(checkpoints_folder, model_name)):\n",
    "        os.mkdir(os.path.join(checkpoints_folder, model_name))\n",
    "    \n",
    "    # optimizers\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "#                                    beta_1=0.9,\n",
    "#                                    beta_2=0.999,\n",
    "#                                    epsilon=1e-07,\n",
    "#                                    amsgrad=False,\n",
    "#                                    name='Adam'\n",
    "#                                   )\n",
    "#     model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    optimiser = \"Adam\" #write your optimizer\n",
    "    model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    print(\"###############################################\")\n",
    "    print(\"Model {} summary\".format(name))\n",
    "    print(model.summary())\n",
    "    print(\"###############################################\")\n",
    "    \n",
    "    # callbacks\n",
    "    filepath = model_name + '-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoints_folder, model_name, filepath),\n",
    "                                                                 monitor='val_loss', \n",
    "                                                                 verbose=1, \n",
    "                                                                 save_best_only=True, \n",
    "                                                                 save_weights_only=False, \n",
    "                                                                 mode='auto', \n",
    "                                                                 period=1) #chekpoints to save model in .h5\n",
    "    LR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, cooldown=5, verbose=1,mode='auto',min_delta=0.00001) #learning Rate\n",
    "    callbacks_list = [checkpoint, LR]\n",
    "    \n",
    "    # calculate step_per_epoch\n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1 #in case of remaining data points which are left after full batches\n",
    "\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1 #in case of remaining data points which are left after full batches\n",
    "\n",
    "    # Training process\n",
    "    print(\"###############################################\")\n",
    "    print(\"Training model\")\n",
    "    \n",
    "    init_epoch = 0\n",
    "    if load_lastest:\n",
    "        model_lastest_path = sorted(glob.glob(os.path.join(checkpoints_folder, model_name, \"*\")))\n",
    "        if len(model_lastest_path) != 0:\n",
    "            model_lastest_path = model_lastest_path[-1]\n",
    "            epoch_lastest = int(os.path.basename(model_lastest_path).split('-')[1])\n",
    "            init_epoch = epoch_lastest\n",
    "            print(\"Loading model from epoch:\", init_epoch)\n",
    "            model = tf.keras.models.load_model(model_lastest_path)\n",
    "    \n",
    "    history=model.fit(train_generator, \n",
    "                      steps_per_epoch=steps_per_epoch, \n",
    "                      epochs=num_epochs, verbose=1, \n",
    "                      callbacks=callbacks_list, \n",
    "                      validation_data=val_generator, \n",
    "                      validation_steps=validation_steps, \n",
    "                      class_weight=None, \n",
    "                      workers=1, \n",
    "                      initial_epoch=init_epoch)\n",
    "    print(\"###############################################\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68621334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy & loss\n",
    "def plot_history(history):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "    axes[0].plot(history.history['loss'])   \n",
    "    axes[0].plot(history.history['val_loss'])\n",
    "    axes[0].legend(['loss','val_loss'])\n",
    "\n",
    "    axes[1].plot(history.history['categorical_accuracy'])   \n",
    "    axes[1].plot(history.history['val_categorical_accuracy'])\n",
    "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525bcfb",
   "metadata": {},
   "source": [
    "## 2.1 3D CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9ad1e",
   "metadata": {},
   "source": [
    "### 2.1.1 C3D\n",
    "C3D inspired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881cba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_3D_01(nb_classes):\n",
    "    \"\"\"\n",
    "        C3D Model\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    x = Conv3D(32,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(inputs)\n",
    "    x = MaxPool3D((1,2,2),strides=(1,2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv3D(128,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv3D(128, (3, 3, 3), strides=(1, 1, 1), padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024,activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128,activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(nb_classes,kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs, x, name = \"3D_01\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f2339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model_3D_01(nb_classes = 5)\n",
    "history = train_model(model, name = '3D_01')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde91e2",
   "metadata": {},
   "source": [
    "## 2.2 Model 2\n",
    "What if 3D CNN go with VGG like (2 conv + 1 pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_3D_02(nb_classes):\n",
    "    \"\"\"\n",
    "        C3D Model\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    # block 1\n",
    "    x = Conv3D(32,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(inputs)\n",
    "    x = Conv3D(32,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((1,2,2),strides=(1,2,2),padding='same')(x)\n",
    "    \n",
    "    # block 2\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "\n",
    "    # block 3\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "    \n",
    "    # block 4\n",
    "    x = Conv3D(128,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv3D(128,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "    \n",
    "    # block 5\n",
    "    x = Conv3D(128, (3, 3, 3), strides=(1, 1, 1), padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024,activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128,activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(nb_classes,kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs, x, name = '3D_02')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_3D_02(nb_classes = 5)\n",
    "history = train_model(model, name = '3D_02')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b05cb0",
   "metadata": {},
   "source": [
    "## 2.3 Custom CNN + LTSM/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D + LTSM/GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22dc218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be97f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer: VGG16/MobileNet/Resnet18/EfficentNet-B0 + LTSM/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60684924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best => use this idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cdea5",
   "metadata": {},
   "source": [
    "## 2.3 Pretrained model with LSTM/GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47aa96a",
   "metadata": {},
   "source": [
    "### 2.3.1 VGG16 + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9daf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_01(nb_classes):\n",
    "    \"\"\"\n",
    "        VGG 16 + GRU\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load VGG16 and freeze it\n",
    "    vgg_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', \n",
    "                                                  include_top=False, \n",
    "                                                  input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    vgg_model.trainable = False\n",
    "    \n",
    "\n",
    "    model = Sequential(name=\"Pretrained_01\")\n",
    "    model.add(TimeDistributed(vgg_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49052f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837c436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_01(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_01')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb0060",
   "metadata": {},
   "source": [
    "### 2.3.2 ResNet50 + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_02(nb_classes):\n",
    "    \"\"\"\n",
    "        ResNet50 + GRU\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load ResNet50 and freeze it\n",
    "    resnet50_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', \n",
    "                                                       include_top=False, \n",
    "                                                       input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    resnet50_model.trainable = False\n",
    "\n",
    "    model = Sequential(name=\"Pretrained_02\")\n",
    "    model.add(TimeDistributed(resnet50_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_02(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_02')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c607289",
   "metadata": {},
   "source": [
    "### 2.3.3 EfficientNetv2B0 + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_03(nb_classes):\n",
    "    \"\"\"\n",
    "        ResNet50 + GRU\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load EfficientNetv2B0 and freeze it\n",
    "    eff_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(weights='imagenet', \n",
    "                                                                       include_top=False, \n",
    "                                                                       input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    eff_model.trainable = False\n",
    "\n",
    "    model = Sequential(name=\"Pretrained_03\")\n",
    "    model.add(TimeDistributed(eff_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_03(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_03')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db711cf2",
   "metadata": {},
   "source": [
    "### 2.3.4 EfficientNetv2B1 + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f408d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_04(nb_classes):\n",
    "    \"\"\"\n",
    "        ResNet50 + GRU\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load EfficientNetv2B1 and freeze it\n",
    "    eff_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B1(weights='imagenet', \n",
    "                                                                       include_top=False, \n",
    "                                                                       input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    eff_model.trainable = False\n",
    "\n",
    "    model = Sequential(name=\"Pretrained_04\")\n",
    "    model.add(TimeDistributed(eff_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_04(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_04')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23520df7",
   "metadata": {},
   "source": [
    "### 2.3.5 MobileNetv2 + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_05(nb_classes):\n",
    "    \"\"\"\n",
    "        MobileNetv2 + GRU\n",
    "    \"\"\"\n",
    "    input_shape = (15, dest_size[0], dest_size[1], 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load MobileNetv2 and freeze it\n",
    "    mob_model = tf.keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet',\n",
    "                                                               include_top=False, \n",
    "                                                               input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    mob_model.trainable = False\n",
    "    model = Sequential(name=\"Pretrained_05\")\n",
    "    model.add(TimeDistributed(mob_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_05(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_05', batch_size=20)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e11bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.98% TRAIN\n",
    "0.90% VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551c7d5",
   "metadata": {},
   "source": [
    "### 2.3.6 Xception + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_06(nb_classes):\n",
    "    \"\"\"\n",
    "        Xception + GRU\n",
    "    \"\"\"\n",
    "    input_shape = (15, dest_size[0], dest_size[1], 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load MobileNetv2 and freeze it\n",
    "    xcep_model = tf.keras.applications.xception.Xception(weights='imagenet',\n",
    "                                                        include_top=False, \n",
    "                                                        input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    xcep_model.trainable = False \n",
    "    \n",
    "    model = Sequential(name=\"Pretrained_06\")\n",
    "    model.add(TimeDistributed(xcep_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fae19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_06(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_06', batch_size=8)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2c7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_07(nb_classes):\n",
    "    \"\"\"\n",
    "        MobileNet + GRU\n",
    "    \"\"\"\n",
    "    num_featuremaps = [16,32,64,128]\n",
    "    num_denselayers = [64,32,5]\n",
    "    num_classes = 5\n",
    "    frames = 15\n",
    "    dropout = 0.25\n",
    "    num_epochs = 30\n",
    "\n",
    "    # increasing batch Size\n",
    "    batch_size = 20\n",
    "\n",
    "    # Input\n",
    "    input_shape=(15, dest_size[0], dest_size[1], 3)\n",
    "\n",
    "    # Define model\n",
    "    mobile_model = tf.keras.applications.mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "    mobile_model.trainable = True\n",
    "\n",
    "    model = Sequential(name = 'Pretrained_07')\n",
    "    model.add(TimeDistributed(mobile_model,input_shape=input_shape))\n",
    "\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(num_denselayers[1],activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(num_classes, activation='softmax')) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadc283",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_07(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_07', batch_size=4)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c4092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
