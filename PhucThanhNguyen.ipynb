{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7233b4df",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "<b> Use Tensorflow 2.10.0 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14f5cd",
   "metadata": {},
   "source": [
    "## 1. Data Loader and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e670de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import os, glob, shutil\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.layers import Dense, Dropout, Conv3D, Input, MaxPool3D, Flatten, Activation, \\\n",
    "                         TimeDistributed, BatchNormalization, MaxPooling2D, LSTM, GRU\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "# set random seed for whole project\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddf4ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting global parameter\n",
    "data_folder = './data'\n",
    "checkpoints_folder = './checkpoints'\n",
    "\n",
    "# setting hyparameter for project\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "dest_size = (128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad7e3a",
   "metadata": {},
   "source": [
    "### 1.1 Read CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d42d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples : 663\n",
      "Total val samples   : 100\n"
     ]
    }
   ],
   "source": [
    "train_doc = np.random.permutation(open('./data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./data/val.csv').readlines()) \n",
    "train_df = pd.read_csv(os.path.join(data_folder, 'train.csv'), delimiter=';', names=['video', 'label (text)', 'label'])\n",
    "val_df   = pd.read_csv(os.path.join(data_folder, 'val.csv'), delimiter=';', names=['video', 'label (text)', 'label'])\n",
    "print('Total train samples :', len(train_df))\n",
    "print('Total val samples   :', len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9e38ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      video    label (text)  label\n",
      "0  WIN_20180925_17_08_43_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "1  WIN_20180925_17_18_28_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "2  WIN_20180925_17_18_56_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "3  WIN_20180925_17_19_51_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "4  WIN_20180925_17_20_14_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "                                      video    label (text)  label\n",
      "0  WIN_20180925_17_17_04_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "1  WIN_20180925_17_43_01_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "2  WIN_20180925_18_01_40_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "3  WIN_20180925_18_03_21_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
      "4  WIN_20180926_16_46_22_Pro_Left_Swipe_new  Left_Swipe_new      0\n"
     ]
    }
   ],
   "source": [
    "# take a look at train and val\n",
    "print(train_df.head(5))\n",
    "print(val_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa1e46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a labels dict with key is numberic and labels is text\n",
    "label_dict = {\n",
    "    0 : 'Left Swipe',\n",
    "    1 : 'Right Swipe',\n",
    "    2 : 'Stop',\n",
    "    3 : 'Thumbs Down',\n",
    "    4 : 'Thumbs Up',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c26a1c",
   "metadata": {},
   "source": [
    "### 1.2 Analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c674db5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = train_df.groupby('label').size()\n",
    "data.plot.bar(xlabel='label', ylabel='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1f00010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = val_df.groupby('label').size()\n",
    "data.plot.bar(xlabel='label', ylabel='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590e9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size (Height and Width of Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889f0d0",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "<li> Data is quite balance </li>\n",
    "<li> Image size is very different between image </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508dbce",
   "metadata": {},
   "source": [
    "### 1.3 Data Generator and Transform\n",
    "Cause data is left and right sensitive, up and down sensitive, so it would be better to not clip, rotate it in spatial\n",
    "Cause data is sequence, so we should not random choose the frame order\n",
    "\n",
    "We can change the contrast and brightness of image\n",
    "https://docs.opencv.org/3.4/d3/dc1/tutorial_basic_linear_transform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e47d1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(im, desired_size = dest_size[0]):\n",
    "    \"\"\"\n",
    "        Resize image with keep aspect ratio\n",
    "    \"\"\"\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    # new_size should be in (width, height) format\n",
    "\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "    delta_w = desired_size - new_size[1]\n",
    "    delta_h = desired_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return new_im\n",
    "\n",
    "def change_brightness(image):\n",
    "    alpha = np.random.uniform(0.75, 1.25)\n",
    "    beta = np.random.uniform(0, 100)\n",
    "    new_image = np.zeros(image.shape, image.dtype)\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            for c in range(image.shape[2]):\n",
    "                new_image[y,x,c] = np.clip(alpha*image[y,x,c] + beta, 0, 255)\n",
    "    return new_image\n",
    "\n",
    "def add_noise(image, threshold = 70):\n",
    "    \"\"\"\n",
    "        Add salt and pepper noise if get random int > threshold\n",
    "    \"\"\"\n",
    "    random_value = np.random.randint(0, 100)\n",
    "    if random_value > threshold:\n",
    "        image = image/255.0\n",
    "        row,col,ch = image.shape\n",
    "        s_vs_p = 0.5\n",
    "        amount = 0.004\n",
    "        out = np.copy(image)\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * image.size * s_vs_p)\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in image.shape]\n",
    "        out[coords] = 1\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in image.shape]\n",
    "        out[coords] = 0\n",
    "        out = image*255\n",
    "        return out\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf0da16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "def generator(source_path, folder_list, batch_size, augmentation = False):\n",
    "    img_idx = [x for x in range(0,30,2)]#create a list of image numbers you want to use for a particular video, we are not using all the 30 images in a video, selected pickup of images\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches =int(len(t)/batch_size) # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            x=len(img_idx)\n",
    "            y=128\n",
    "            z=128\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            cnt_img=0\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                cnt_img+=1\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    if augmentation:\n",
    "                        image = change_brightness(image)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=resize_img(image)\n",
    "                    image=image/255.0\n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        cnt_img=0\n",
    "        batch_cover = num_batches*batch_size\n",
    "        rem = len(t) - batch_cover\n",
    "        if(len(t)!=batch_cover):\n",
    "            batch_data = np.zeros((rem,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((rem,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(rem):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + batch_cover].split(';')[0]) # read all the images in the folder\n",
    "                cnt_img+=1\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in  \n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + batch_cover].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=resize_img(image)\n",
    "                    image=image/255.0\n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + batch_size].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47b0f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 50\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = './data/train'\n",
    "val_path = './data/val'\n",
    "num_train_sequences = len(train_df)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_df)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de1eb4",
   "metadata": {},
   "source": [
    "## Model buiding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd2cd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, name, num_epochs = num_epochs, batch_size = batch_size, load_lastest = True):\n",
    "    # train and val generator\n",
    "    train_generator = generator(train_path, train_doc, batch_size)\n",
    "    val_generator = generator(val_path, val_doc, batch_size)\n",
    "    \n",
    "    # create folder for saving checkpoints\n",
    "    model_name = 'model' + '_' + name\n",
    "\n",
    "    if not os.path.exists(os.path.join(checkpoints_folder, model_name)):\n",
    "        os.mkdir(os.path.join(checkpoints_folder, model_name))\n",
    "    \n",
    "    # optimizers\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                   beta_1=0.9,\n",
    "                                   beta_2=0.999,\n",
    "                                   epsilon=1e-07,\n",
    "                                   amsgrad=False,\n",
    "                                   name='Adam'\n",
    "                                  )\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    print(\"###############################################\")\n",
    "    print(\"Model {} summary\".format(name))\n",
    "    print(model.summary())\n",
    "    print(\"###############################################\")\n",
    "    \n",
    "    # callbacks\n",
    "    filepath = model_name + '-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoints_folder, model_name, filepath),\n",
    "                                                                 monitor='val_loss', \n",
    "                                                                 verbose=1, \n",
    "                                                                 save_best_only=True, \n",
    "                                                                 save_weights_only=False, \n",
    "                                                                 mode='auto', \n",
    "                                                                 period=1) #chekpoints to save model in .h5\n",
    "    LR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=5, cooldown=4, verbose=1,mode='auto',min_delta=0.0001) #learning Rate\n",
    "    EL = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
    "    callbacks_list = [checkpoint, LR, EL]\n",
    "    \n",
    "    # calculate step_per_epoch\n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1 #in case of remaining data points which are left after full batches\n",
    "\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1 #in case of remaining data points which are left after full batches\n",
    "\n",
    "    # Training process\n",
    "    print(\"###############################################\")\n",
    "    print(\"Training model\")\n",
    "    \n",
    "    init_epoch = 0\n",
    "    if load_lastest:\n",
    "        model_lastest_path = sorted(glob.glob(os.path.join(checkpoints_folder, model_name, \"*\")))\n",
    "        if len(model_lastest_path) != 0:\n",
    "            model_lastest_path = model_lastest_path[-1]\n",
    "            epoch_lastest = int(os.path.basename(model_lastest_path).split('-')[1])\n",
    "            init_epoch = epoch_lastest\n",
    "            print(\"Loading model from epoch:\", init_epoch)\n",
    "            model = tf.keras.models.load_model(model_lastest_path)\n",
    "    \n",
    "    history=model.fit(train_generator, \n",
    "                      steps_per_epoch=steps_per_epoch, \n",
    "                      epochs=num_epochs, verbose=1, \n",
    "                      callbacks=callbacks_list, \n",
    "                      validation_data=val_generator, \n",
    "                      validation_steps=validation_steps, \n",
    "                      class_weight=None, \n",
    "                      workers=1, \n",
    "                      initial_epoch=init_epoch)\n",
    "    print(\"###############################################\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68621334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy & loss\n",
    "def plot_history(history):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "    axes[0].plot(history.history['loss'])   \n",
    "    axes[0].plot(history.history['val_loss'])\n",
    "    axes[0].legend(['loss','val_loss'])\n",
    "\n",
    "    axes[1].plot(history.history['categorical_accuracy'])   \n",
    "    axes[1].plot(history.history['val_categorical_accuracy'])\n",
    "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7495ce",
   "metadata": {},
   "source": [
    "## 2.1 3D CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9ad1e",
   "metadata": {},
   "source": [
    "### 2.1.1 C3D\n",
    "C3D inspired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "881cba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_3D_01(nb_classes):\n",
    "    \"\"\"\n",
    "        C3D Model\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    x = Conv3D(32,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(inputs)\n",
    "    x = MaxPool3D((1,2,2),strides=(1,2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv3D(128,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv3D(128, (3, 3, 3), strides=(1, 1, 1), padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024,activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128,activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(nb_classes,kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs, x, name = \"3D_01\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f2339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "Model 3D_01 summary\n",
      "Model: \"3D_01\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 15, 128, 128, 3)  0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 15, 128, 128, 32)  2624      \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 15, 64, 64, 32)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 15, 64, 64, 64)    55360     \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 8, 32, 32, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 8, 32, 32, 64)     110656    \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 4, 16, 16, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 4, 16, 16, 128)    221312    \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 2, 8, 8, 128)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 2, 8, 8, 128)      442496    \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPooling  (None, 1, 4, 4, 128)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,062,469\n",
      "Trainable params: 3,062,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "###############################################\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "###############################################\n",
      "Training model\n",
      "Epoch 1/50\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.4535 - categorical_accuracy: 0.1614\n",
      "Epoch 1: val_loss improved from inf to 2.15864, saving model to ./checkpoints\\model_3D_01\\model_3D_01-00001-4.45348-0.16139-2.15864-0.19000.h5\n",
      "83/83 [==============================] - 21s 240ms/step - loss: 4.4535 - categorical_accuracy: 0.1614 - val_loss: 2.1586 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.8901 - categorical_accuracy: 0.2097"
     ]
    }
   ],
   "source": [
    "model = Model_3D_01(nb_classes = 5)\n",
    "history = train_model(model, name = '3D_01')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde91e2",
   "metadata": {},
   "source": [
    "## 2.2 Model 2\n",
    "What if 3D CNN go with VGG like (2 conv + 1 pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_3D_02(nb_classes):\n",
    "    \"\"\"\n",
    "        C3D Model\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    # block 1\n",
    "    x = Conv3D(32,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(inputs)\n",
    "    x = Conv3D(32,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((1,2,2),strides=(1,2,2),padding='same')(x)\n",
    "    \n",
    "    # block 2\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "\n",
    "    # block 3\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "    \n",
    "    # block 4\n",
    "    x = Conv3D(128,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv3D(128,(3,3,3),strides=(1,1,1),padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)\n",
    "    \n",
    "    # block 5\n",
    "    x = Conv3D(128, (3, 3, 3), strides=(1, 1, 1), padding='same',\n",
    "               activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPool3D((2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024,activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128,activation='relu',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(nb_classes,kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs, x, name = '3D_02')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_3D_02(nb_classes = 5)\n",
    "history = train_model(model_02, name = '3D_02')\n",
    "plot_history(history_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b05cb0",
   "metadata": {},
   "source": [
    "## 2.3 Custom CNN + LTSM/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D + LTSM/GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22dc218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be97f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer: VGG16/MobileNet/Resnet18/EfficentNet-B0 + LTSM/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60684924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best => use this idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cdea5",
   "metadata": {},
   "source": [
    "## 2.3 Pretrained model with LSTM/GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef495425",
   "metadata": {},
   "source": [
    "### 2.3.1 VGG16 + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_01(nb_classes):\n",
    "    \"\"\"\n",
    "        VGG 16 + LSTM\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load VGG16 and freeze it\n",
    "    vgg_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', \n",
    "                                                  include_top=False, \n",
    "                                                  input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    vgg_model.trainable = False\n",
    "\n",
    "    model = Sequential(name=\"Pretrained_01\")\n",
    "    model.add(TimeDistributed(vgg_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_01(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_01')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32589be6",
   "metadata": {},
   "source": [
    "### 2.3.2 ResNet50 + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_02(nb_classes):\n",
    "    \"\"\"\n",
    "        ResNet50 + LSTM\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load ResNet50 and freeze it\n",
    "    resnet50_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', \n",
    "                                                       include_top=False, \n",
    "                                                       input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    resnet50_model.trainable = False\n",
    "\n",
    "    model = Sequential(name=\"Pretrained_02\")\n",
    "    model.add(TimeDistributed(resnet50_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_02(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_02')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f38cdd",
   "metadata": {},
   "source": [
    "### 2.3.3 EfficientNetv2B0 + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_03(nb_classes):\n",
    "    \"\"\"\n",
    "        ResNet50 + LSTM\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load EfficientNetv2B0 and freeze it\n",
    "    eff_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(weights='imagenet', \n",
    "                                                                       include_top=False, \n",
    "                                                                       input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    eff_model.trainable = False\n",
    "\n",
    "    model = Sequential(name=\"Pretrained_03\")\n",
    "    model.add(TimeDistributed(eff_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_03(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_03')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82b704",
   "metadata": {},
   "source": [
    "### 2.3.4 EfficientNetv2B1 + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe45188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_04(nb_classes):\n",
    "    \"\"\"\n",
    "        ResNet50 + LSTM\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load EfficientNetv2B1 and freeze it\n",
    "    eff_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B1(weights='imagenet', \n",
    "                                                                       include_top=False, \n",
    "                                                                       input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    eff_model.trainable = False\n",
    "\n",
    "    model = Sequential(name=\"Pretrained_04\")\n",
    "    model.add(TimeDistributed(eff_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_04(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_04')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d24e2d",
   "metadata": {},
   "source": [
    "### 2.3.5 MobileNetv2 + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_05(nb_classes):\n",
    "    \"\"\"\n",
    "        MobileNetv2 + LSTM\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load MobileNetv2 and freeze it\n",
    "    mob_model = tf.keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet',\n",
    "                                                               include_top=False, \n",
    "                                                               input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    mob_model.trainable = False\n",
    "    \n",
    "    model = Sequential(name=\"Pretrained_05\")\n",
    "    model.add(TimeDistributed(mob_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_05(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_05', batch_size=4)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f170ea",
   "metadata": {},
   "source": [
    "### 2.3.6 Xception + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Pretrained_06(nb_classes):\n",
    "    \"\"\"\n",
    "        Xception + LSTM\n",
    "    \"\"\"\n",
    "    input_shape = (15, 128, 128, 3)\n",
    "    weight_decay = 0.005\n",
    "    nb_classes = nb_classes\n",
    "    \n",
    "    # load MobileNetv2 and freeze it\n",
    "    xcep_model = tf.keras.applications.xception.Xception(weights='imagenet',\n",
    "                                                        include_top=False, \n",
    "                                                        input_shape=(dest_size[0], dest_size[1], 3))\n",
    "    xcep_model.trainable = False\n",
    "    \n",
    "    model = Sequential(name=\"Pretrained_06\")\n",
    "    model.add(TimeDistributed(xcep_model,input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # using Softmax as last layer\n",
    "    model.add(Dense(nb_classes, activation='softmax')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe420c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Pretrained_06(nb_classes = 5)\n",
    "history = train_model(model, name = 'Pretrained_06', batch_size=4)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edace7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
